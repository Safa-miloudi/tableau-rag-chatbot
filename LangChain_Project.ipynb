{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. INSTALL DEPENDENCIES & Import Libraries:**"
      ],
      "metadata": {
        "id": "a7Zo45gPhK7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NSTALL DEPENDENCIES: 'langchain-google-genai'\n",
        "!pip install -q langchain-community langchain-text-splitters langchain-google-genai langchain-chroma pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ1nvhNJhbza",
        "outputId": "9c92ef8e-a11f-400b-d7e6-b97812bb0f32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "#LangChain Imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, TokenTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# Google GenAI Imports\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "jPNYNW7Bh1r-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. SETUP API KEY**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eQBvVKfQhsGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY"\n",
        "\n",
        "if not os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\") == \"YOUR_NEW_API_KEY_HERE\":\n",
        "    raise ValueError(\"⚠️ Please replace the placeholder with your actual Google API key.\")\n",
        "else:\n",
        "    print(\"✅ Gemini API Key set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRIpyeYjjLLZ",
        "outputId": "fd886d53-3268-4575-ec31-b0bc9979add5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gemini API Key set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. DATA LOADING**"
      ],
      "metadata": {
        "id": "o_q0A-s1jReP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_documents(pdf_path):\n",
        "    print(f\"Checking for file at: {pdf_path}\")\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"❌ File not found at {pdf_path}. \"\n",
        "            f\"Please ensure 'Introduction_to_Tableau.pdf' is uploaded to the Colab '/content/' folder.\"\n",
        "        )\n",
        "\n",
        "    print(\"File found! Loading...\")\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    raw_documents = loader.load()\n",
        "\n",
        "    # Merge pages for processing\n",
        "    full_text = \"\".join([doc.page_content for doc in raw_documents])\n",
        "\n",
        "    # Split by Headers\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=[(\"#\", \"Section Title\"), (\"##\", \"Lecture Title\")]\n",
        "    )\n",
        "    md_docs = markdown_splitter.split_text(full_text)\n",
        "\n",
        "    # Split by Tokens\n",
        "    token_splitter = TokenTextSplitter(\n",
        "        encoding_name=\"cl100k_base\",\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50\n",
        "    )\n",
        "    token_docs = token_splitter.split_documents(md_docs)\n",
        "\n",
        "    print(f\"✅ Successfully loaded and created {len(token_docs)} chunks.\")\n",
        "    return token_docs\n",
        "\n",
        "# Make sure your file is uploaded here!\n",
        "pdf_path = \"/content/Introduction_to_Tableau.pdf\"\n",
        "documents = load_and_split_documents(pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzZ4Lo3gja1M",
        "outputId": "2920e192-9dfe-470f-b143-60e96c6682c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for file at: /content/Introduction_to_Tableau.pdf\n",
            "File found! Loading...\n",
            "✅ Successfully loaded and created 43 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. VECTOR STORE & EMBEDDINGS**"
      ],
      "metadata": {
        "id": "fi-ePpfjjkjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
        "persist_directory = \"./chroma_db\"\n",
        "\n",
        "print(\"Creating Vector Store (this may take a moment)...\")\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "print(\"Vector Store created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0ErU1_Cj2Da",
        "outputId": "dc38c9b2-b3fa-4440-aa6c-b198305e203c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Vector Store (this may take a moment)...\n",
            "Vector Store created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. RAG CHAIN**"
      ],
      "metadata": {
        "id": "9iGChCxPj7_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 5, \"lambda_mult\": 0.8}\n",
        ")\n",
        "\n",
        "template = \"\"\"\n",
        "You are a helpful Q&A chatbot for the 'Introduction to Tableau' course.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "At the end of your answer, explicitly list the 'Lecture Title' and 'Section Title'\n",
        "from the metadata of the context you used.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Using Gemini 2.5 Flash\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "EgTBd2M4kC5_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. CHAT FUNCTION**"
      ],
      "metadata": {
        "id": "Et75PQNvkKiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(question):\n",
        "    print(f\"\\nQuestion: {question}\\nAnswer: \", end=\"\", flush=True)\n",
        "    # Stream the response natively with Gemini\n",
        "    for chunk in rag_chain.stream(question):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "# Test Questions\n",
        "chat_with_bot(\"How do I create a calculated field in Tableau?\")\n",
        "chat_with_bot(\"What is the difference between a join and a blend?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCe4yvTtkYbR",
        "outputId": "e04915dc-7372-4ee9-9b59-9f97d4a090d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: How do I create a calculated field in Tableau?\n",
            "Answer: To create a calculated field in Tableau, follow these steps:\n",
            "\n",
            "1.  Go to the **Analysis tab**.\n",
            "2.  Select **Create Calculated Field**.\n",
            "3.  A dialog box will open.\n",
            "4.  Type the **name** of your new field.\n",
            "5.  Define your calculation using Tableau's operators, functions, and symbols. You can use:\n",
            "    *   **Operators:** `+` (addition), `-` (subtraction), `*` (multiplication), `/` (division), `^` (elevation).\n",
            "    *   **Comparison Operators:** `=`, `>`, `<`, `>=`, `!=` (different).\n",
            "    *   **Logical Functions:** `AND`, `OR`, `NOT`.\n",
            "    *   **Common Functions:** `SUM`, `AVERAGE`, `MIN`, `MAX`, `ABS`.\n",
            "    *   **Text Functions:** `LEFT`, `RIGHT`, `MID`, `UPPER`, `LOWER`.\n",
            "    *   **Counting Functions:** `COUNT` (e.g., to count the number of ratings or purchases).\n",
            "6.  Tableau provides **autocomplete suggestions** as you type and indicates whether your calculation is **valid** at the bottom of the dialog box.\n",
            "\n",
            "**Lecture Title:** Introduction to Tableau\n",
            "**Section Title:** Adding a custom calculation\n",
            "--------------------------------------------------\n",
            "\n",
            "Question: What is the difference between a join and a blend?\n",
            "Answer: Data blending and joining are both methods for combining data in Tableau, but they differ in several key aspects:\n",
            "\n",
            "*   **Automation vs. Manual:** Data blending is a functionality that Tableau implements automatically while you are working on your sheet, making it an intuitive feature. In contrast, joining data is something you do manually.\n",
            "*   **Aggregation Location:** When blending, Tableau takes separate query results from each data source, aggregates them in the view (in Tableau), and *then* connects and joins the aggregated query results on a common field. With joining, the aggregation happens at the database level, and only the output of the join is brought back to Tableau.\n",
            "*   **Handling of Duplicates and Inflated Data:** If two data tables containing duplicate values are joined without proper preliminary aggregation, it can lead to an artificially inflated dataset in Tableau. Blending automatically solves this problem by considering the level of granularity chosen in the view and combining data sources with aggregated fields directly.\n",
            "*   **Data Source Requirements:** Blending requires both data tables to be separate data sources. Joining can combine tables within the same data source or across different ones.\n",
            "*   **Type of Combination:** A data blend can be thought of as a specific type of left join. Joins, however, offer various types such as inner, left, right, and outer joins, allowing for different ways of combining rows based on matching keys.\n",
            "*   **Problem Solving:** Blending is often preferred or necessary when joins lead to issues, such as date fields from two joined tables not matching up correctly, or when an inflated dataset is produced.\n",
            "\n",
            "Both methods require a common field to create the connection between the two data tables.\n",
            "\n",
            "Lecture Title: Not available in the provided context.\n",
            "Section Title: Not available in the provided context.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
